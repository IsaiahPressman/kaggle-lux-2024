max_updates: null
optimizer_kwargs:
  lr: 1e-4
  eps: 0.000_3
lr_schedule:
  steps: 150_000
  min_factor: 0.25
clip_gradients: 10.0
steps_per_update: 32
epochs_per_update: 1
train_batch_size: 128
use_mixed_precision: true
teacher_path: null

gamma: 0.9999
gae_lambda: 0.85
clip_coefficient: 0.2
loss_coefficients:
  policy: 1.0
  value: 0.5
  entropy: 1e-4
  # teacher_policy and teacher_value don't have any effect unless
  # teacher_path is set above
  teacher_policy:
    start: 1.0
    end: 5e-4
    steps: 10_000
  teacher_value:
    start: 0.5
    end: 0.0
    steps: 10_000

env_config:
  n_envs: 64
  frame_stack_len: 10
  # POINT_TILES or OPP_UNIT_FRONTIER
  sap_masking: POINT_TILES
  # FINAL_WINNER, MATCH_WINNER, or POINTS_SCORED
  reward_space: MATCH_WINNER
  jax_device: cpu:0

rl_model_config:
  model_arch: conv
  d_model: 64
  n_blocks: 4

device: cuda:0
eval_games: 512
eval_device: cuda:1
log_histograms: false
